{"name":"stat_agg","tagline":"statistical aggregates for machine learning in python","body":"# Description\r\n\r\nstat\\_agg is a simple Python package for aggregating predictions from an ensemble\r\nof learners. When aggregated, the statistical accuracy of predictions\r\nfrom different learners is often greater than any one of them. This package implements\r\nvarious approaches to aggregated prediction of continuous and categorical \r\nprediction challenges.\r\n\r\n\r\nThe goal of the stat\\_agg package is to:\r\n\r\n1. Provide a suite of statistical aggregators that maximize ensemble\r\nprediction accuracy for continuous and categorical outcomes.\r\n2. Manage ensemble in a way that is dynamic. New learners\r\ncan be added to an ensemble at any time.\r\n3. Detect and retrain when one or more of the learners suddenly becomes\r\nunavailable.\r\n\r\n# Requirements\r\n\r\nThe stat\\_agg package has been tested on Python version 2.7 with the following\r\npackages:\r\n- pandas 0.17.1\r\n- sklearn 0.17\r\n\r\nInstalling stat\\_agg\r\n---\r\n\r\nThe easiest way to install stat\\_agg is to use pip from within a shell:\r\n\r\n```bash\r\n> pip install -e git+https://github.com/kaneplusplus/stat_agg.git#egg=statagg\r\n```\r\n\r\n# Support\r\n\r\nstatagg is supported on Python version 2.7\r\n\r\nThe development home of this project can be found at: [https://github.com/kaneplusplus/stat\\_agg](https://github.com/kaneplusplus/stat\\_agg)\r\n\r\nThe package currently supports the following statistical aggregators:\r\n- Categorical Prediction \r\n    - Max Vote\r\n    - Min Vote\r\n    - Minimum Variance\r\n- Continuous Prediction\r\n    - Average Value\r\n    - Minimum Variance\r\n    - Least Squares\r\n    - Random Forests\r\n\r\n# Using the library\r\n\r\n## Simple Aggregators\r\n\r\nSimple aggregators are those where no training is necessary and the aggregate\r\nprediction can be calculated directly from learners. One example of this is\r\nmajority vote where the aggregator simply returns the predition that appears the\r\nmost often. An example is shown below.\r\n\r\n```{Python}\r\nfrom statagg import *\r\n# Create 2 learners named '1' and '2' with 2 predictions each.\r\nprediction_data = {'1': ['a', 'a'], '3': ['b', 'a']}\r\nmv = MajorityVote()\r\nprint(mv.predict(prediction_data))\r\n# [None, 'a']\r\n```\r\nNote that in the first prediction, learner 1 and 2 predicted ```'a'``` and \r\n```'b'``` respectively. Since there is no majority in this case, a value of\r\n```None``` was returned. Other simple aggregators include minority vote for\r\ncategorical variables and average for continuous outcomes.\r\n\r\n## Model-based Aggregators\r\n\r\nMore sophisticated aggregators can be constructed by training on the\r\naccuracy of learners' predictions. One example is the ordinary least squares (OLS)\r\naggregator, which uses learner predictions as regressors and fits against the\r\noutcome. An example using the iris data set is shown below.\r\n\r\n```{Python}\r\nfrom statagg import *\r\nfrom pandas import read_csv\r\nfrom statistics import pstdev, variance\r\nimport statsmodels.formula.api as sm\r\n\r\niris_url = \"https://raw.githubusercontent.com/pydata/pandas/master/panda s/tests/data/iris.csv\" \r\n\r\n# Download the iris data set.\r\niris = read_csv(iris_url)\r\n\r\n# Partition iris into 3 parts.\r\niris1 = iris[0:15].append(iris[50:65]).append(iris[100:115])\r\niris2 = iris[15:40].append(iris[65:90]).append(iris[115:140])\r\niris3 = iris[40:50].append(iris[90:100]).append(iris[140:150])\r\n\r\n# Fit the iris subsets using the statsmodels package..\r\nform = \"SepalLength ~ SepalWidth + PetalLength + PetalWidth + Name\"\r\nfit1 = sm.ols(formula=form, data=iris1).fit()\r\nfit2 = sm.ols(formula=form, data=iris2).fit()\r\nfit3 = sm.ols(formula=form, data=iris3).fit()\r\n\r\n# Get a random subset of the iris data.\r\niris_sample = iris.sample(50)\r\n\r\nest1 = fit1.predict(iris_sample)\r\nest2 = fit2.predict(iris_sample)\r\nest3 = fit3.predict(iris_sample)\r\n\r\n# Print the in-sample standard errors.\r\nprint pstdev(est1 - iris_sample['SepalLength'])\r\n# 0.3014955119\r\nprint pstdev(est2 - iris_sample['SepalLength'])\r\n# 0.279841460366\r\nprint pstdev(est3 - iris_sample['SepalLength'])\r\n# 0.363993665693\r\n\r\ntraining_data = {\"prediction\" : {'a': est1,\r\n                                 'b': est2,\r\n                                 'c': est3},\r\n                 \"actual\" : iris_sample['SepalLength']}\r\n\r\n# Use the training data to fit the OLS aggregator.\r\nmco = MinimumContinuousOLS()\r\nmco.train(training_data)\r\n\r\n# Print the standard deviation of the aggregator.\r\nprint pstdev(mco.predict(training_data['prediction']) - \\\r\n                         iris_sample['SepalLength'])\r\n# 0.271979762123\r\n```\r\nThe OLS aggregator provides a small increase of in-sample variance. Other model-based\r\naggregators include minimum variance (for both categorical and continuous outcomes)\r\nand a random forests aggregator.\r\n\r\n# Benchmarks\r\n\r\n_coming by the end of Dec. 30 2015_\r\n\r\n# Contact\r\n\r\nContributions are welcome.\r\n\r\nFor more information contact Michael Kane at [kaneplusplus@gmail.com](kaneplusplus@gmail.com).\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}