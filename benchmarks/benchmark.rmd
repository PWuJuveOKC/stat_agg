# Benchmark

The stat_agg package was intented to be used for an ensemble of 
heterogeneous learners. It should be noted that this is slightly different
than the ensemble learners provided in packages like sklearn, where each
learner resamples a single data set and/or the parameters for each learner are
varied. stat_agg makes no assumptions about either the learners or
the data sets from which the predictions are calculated. It only assumes
that predictions are for the same outcome. Furthermore, we are not aware
of other packages of this kind. We speculate that statistical 
aggregation of heterogeneous learners is generally done ad-hoc and part of the
intention of this package is to leverage sklearn for both the creation
of the individual learners as well as their aggregate prediction. 

As a result,the benchmarks here show the performance for a simple use case;
namely, prediction airline delays. The data set used will be 2008 flight delay 
information for those carriers with at least 1% commercial U.S. domestic flights.
The data set contains information for 7,009,729 flights. Each flight contains 
29 variables related to flight time, delay time, departure airport, arrival 
airport, and so on.

## Reproducing the Results

All scripts required to run the benchmarks are available in the [stat_agg
repository](https://github.com/kaneplusplus/stat_agg/tree/master/benchmarks). The directory contains:

- [make_data.r](https://github.com/kaneplusplus/stat_agg/blob/master/benchmarks/make_data.r) - an R script for downloading the data, creating training and
test data, fitting the learners (for both regression and classification),
and outputting their predictions.
- [benchmark_regression.py](https://github.com/kaneplusplus/stat_agg/blob/master/benchmarks/benchmark_regression.py) - a Python script that runs the regression 
benchmarks.
- [benchmark_classification.py](https://github.com/kaneplusplus/stat_agg/blob/master/benchmarks/benchmark_classification.py) - a Python script that runs the 
classification benchmarks.
- [benchmark.rmd](https://github.com/kaneplusplus/stat_agg/blob/master/benchmarks/benchmark.rmd) - an R markdown file for generating this file.

To create the files needed for the benchmark you can simply either source 
make_data.r or call it from the terminal.
```{bash}
Rscript make_data.r
```

## The Benchmark Datasets

The following data sets are created when the make\_data.r script is run:

- regression_predictions_train.csv - the in-sample predictions of
the departure delay of 6 learners. All learners were fitted using a 
random subset of 5 variables and least squares regression. 
- regression_predictions_test.csv - out of sample predictions of the departure
delay and a extra column giving the actual departure delay.
- classification_prediction_train.csv - the in-sample predictions of whether
or not a flight was at left late (the departure delay is greater than or
equal to 15 minutes) of 6 learners. All learners were fitted using a
random subset of 5 variables and all were fitted using logistic regression.
- classification_prediction_test.csv - out of sample predictions of late
departures and an extra column giving the actual departure delay.

## Regression: Predicting Departure Delays

The benchmark_regression.py script benchmarks the aggregation of continuous
variables using least squares. The package is trained on a data set where
each of the learners is represented. However, if variables are not present
for the prediction, whether it is because the learner is not available or
because it is taking too long, the internal weighting of the learners is
recalculated. The timing for least squares can be tested simply by running 
the script.

```{bash}
python benchmark_regression.py
```

```{r, fig.height=800, fig.width=400, echo=FALSE, eval=FALSE}
library(rbokeh)
library(foreach)
x = read.csv("ols_pred.csv")
mse = foreach(j=3:11, .combine=c) %do% {
  var(x[,j]-x$actual)
}
y = read.csv("regression_predictions_test.csv")
indiv_mse = foreach(j=1:10, .combine=c) %do% { 
  var(y[,j]-y$actual)
}
figure(width=800, height=600) %>% ly_points(mse) %>% 
  x_axis("Number of Learners") %>% y_axis("Mean Square Error") %>%
  ly_abline(a=min(indiv_mse), b=0)
```

![figures/mse/index.html]

The plot above shows the accuracy of the aggregate predictor in the number
of learners used to create the aggregate. The horizontal red line shows the
best accuracy of any individual learner. It is importatnt to aggregates are not guaranteed to improve accuracy. In general 
the accuracy first increases but past a given point, more learners 
may actually decrease accuracy. As a result the number of learners must
be tuned for the application. However, when tuned they generally provide
accuracy well beyond the accuracy of any given predictor. 

## Classification: Predicting Lateness

```{bash}
python benchmark_classification.py
```
```{r, fig.height=800, fig.width=400, echo=FALSE}
library(rbokeh)
library(foreach)
x = read.csv("class_pred.csv")
misclass_rate = foreach(j=3:11, .combine=c) %do% {
  sum(x[,j] != x$actual, na.rm=TRUE)/nrow(x)
}
y = read.csv("classification_predictions_test.csv")
indiv_misclass = foreach(j=1:10, .combine=c) %do% { 
  sum(y[,j] != y$actual, na.rm=TRUE)/nrow(y)
}

figure(width=800,height=600,ylim=range(c(misclass_rate,min(indiv_misclass)))) %>% 
  ly_points(misclass_rate) %>% 
  x_axis("Number of Learners") %>% y_axis("Misclassification Rate") %>%
  ly_abline(a=min(indiv_misclass), b=0, col="red")
```
The plot above shows the misclassification rate of the aggregates
in the number of learners.
